{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install --upgrade albumentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qs453tkgdf3h",
    "outputId": "f4e53f6f-2c33-413e-ec98-c7fc4825ccce"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms, models\n",
    "\n",
    "from albumentations import HorizontalFlip, RandomBrightnessContrast, Resize, Compose\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import cv2 as cv\n",
    "from PIL import Image\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "svh-kWQJk5Z7",
    "outputId": "2328ed8b-b5b9-41f4-f8eb-07914807048f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraction complete!\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "\n",
    "# Path to the ZIP file\n",
    "zip_path = r\"C:\\Users\\4312239\\Downloads\\archive (3).zip\"\n",
    "extract_to = \"cityscapes\"  # Folder where files will be extracted\n",
    "\n",
    "# Open the ZIP file and extract its contents\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_to)\n",
    "\n",
    "print(\"Extraction complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "5HChtDsKjDF8"
   },
   "outputs": [],
   "source": [
    "# Dataset paths\n",
    "DATASET_DIR = r\"C:\\Users\\4312239\\Downloads\\cityscapes\"\n",
    "\n",
    "train_images = os.path.join(DATASET_DIR, \"train/img\")\n",
    "train_labels = os.path.join(DATASET_DIR, \"train/label\")\n",
    "\n",
    "val_images = os.path.join(DATASET_DIR, \"val/img\")\n",
    "val_labels = os.path.join(DATASET_DIR, \"val/label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Jkiwlk1DnN4V"
   },
   "outputs": [],
   "source": [
    "# Define custom dataset\n",
    "class SegmentationDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, img_dir, lbl_dir, augmentations=None):\n",
    "\n",
    "        # Path to the folder containing images & masks\n",
    "        self.img_dir = img_dir\n",
    "        self.lbl_dir = lbl_dir\n",
    "\n",
    "        self.augmentations = augmentations\n",
    "\n",
    "        # Store the sorted list of filenames for images and masks to ensure they are correctly paired\n",
    "        self.images = sorted(os.listdir(img_dir))\n",
    "        self.labels = sorted(os.listdir(lbl_dir))\n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        # Returns the number of images\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.images[idx]) # Get image path\n",
    "        lbl_path = os.path.join(self.lbl_dir, self.labels[idx]) # Get label path\n",
    "        img = cv.imread(img_path) # Read the image\n",
    "        lbl = cv.imread(lbl_path, cv.IMREAD_GRAYSCALE) # Read the label in grayscale\n",
    "\n",
    "        # If augmentations are provided\n",
    "        if self.augmentations:\n",
    "            augmented = self.augmentations(image=img, mask=lbl)\n",
    "            img, lbl = augmented[\"image\"], augmented[\"mask\"]\n",
    "\n",
    "        # divide by 255 to normalize them into the range [0, 1]\n",
    "        img = img.float()/255.0\n",
    "\n",
    "        return img, lbl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "V2zjEUHOjG5j"
   },
   "outputs": [],
   "source": [
    "# preprocessing and augmentation\n",
    "def get_augmentations(train=True):\n",
    "    if train:\n",
    "        return Compose([\n",
    "            Resize(96, 256),  # Resize images to (256, 512)\n",
    "            HorizontalFlip(p=0.5), # Flip images horizontally with 50% probability\n",
    "            RandomBrightnessContrast(p=0.2), # Randomly change brightness/contrast with 20% probability\n",
    "            ToTensorV2() # Convert image/mask to PyTorch tensors\n",
    "        ])\n",
    "    else: # No data augmentation for validation (Only preprocessing)\n",
    "        return Compose([\n",
    "            Resize(96, 256),\n",
    "            ToTensorV2()\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "8eQRQzD9m-L3"
   },
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "train_augmentations = get_augmentations(train=True)\n",
    "val_augmentations = get_augmentations(train=False)\n",
    "\n",
    "train_dataset = SegmentationDataset(train_images, train_labels, augmentations=train_augmentations)\n",
    "val_dataset = SegmentationDataset(val_images, val_labels, augmentations=val_augmentations)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "l0TSk9NtmYdJ"
   },
   "outputs": [],
   "source": [
    "# Define the Fully Convolutional Network (FCN)\n",
    "# VGG16 as a feature extractor (encoder) & custom decoder to reconstructs the segmentation\n",
    "class FCN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(FCN, self).__init__()\n",
    "        backbone = models.vgg16(weights=models.VGG16_Weights.DEFAULT)\n",
    "\n",
    "        # remove fully connected layers & keep only convolutional layers\n",
    "        self.encoder = backbone.features\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "          # nn.ConvTranspose2d(Input channels, Output channels, kernel_size, stride, padding, output_padding)\n",
    "\n",
    "            nn.ConvTranspose2d(512, 256, 3, (2, 1), 1, (1, 0)),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(256, 128, 3, (2, 1), 1, (1, 0)),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(128, 64, 3, (2, 1), 1, (1, 0)),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            # reduce the number of channels to match the number of segmentation classes\n",
    "            nn.Conv2d(64, num_classes, 1)\n",
    "        )\n",
    "\n",
    "    # x = input image [batch_size, channels, height, width]\n",
    "    def forward(self, x):\n",
    "\n",
    "        # Extracts features using VGG16 (Encoder)\n",
    "        features = self.encoder(x)\n",
    "\n",
    "        # Passing features through the Decoder\n",
    "        segmentation_map = self.decoder(features)\n",
    "        segmentation_map = nn.functional.interpolate(segmentation_map, size=(96, 256), mode='bilinear', align_corners=False)\n",
    "\n",
    "        return segmentation_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YpviHHKkoAQp",
    "outputId": "e4f8dbe8-700c-449b-ae68-a7d763dd5527"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FCN(\n",
      "  (encoder): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): ReLU(inplace=True)\n",
      "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): ReLU(inplace=True)\n",
      "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): ReLU(inplace=True)\n",
      "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (20): ReLU(inplace=True)\n",
      "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (22): ReLU(inplace=True)\n",
      "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (27): ReLU(inplace=True)\n",
      "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (29): ReLU(inplace=True)\n",
      "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): ConvTranspose2d(512, 256, kernel_size=(3, 3), stride=(2, 1), padding=(1, 1), output_padding=(1, 0))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(2, 1), padding=(1, 1), output_padding=(1, 0))\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 1), padding=(1, 1), output_padding=(1, 0))\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): Conv2d(64, 19, kernel_size=(1, 1), stride=(1, 1))\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = FCN(num_classes=19)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "95SteuPBoXN5",
    "outputId": "51899447-00dc-4617-af74-e2d8fe830e50"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "8yTFCPreoZZm"
   },
   "outputs": [],
   "source": [
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5IBMVKMQobKI",
    "outputId": "057261a7-2e13-4d5e-f757-274e596c69ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.2123\n",
      "Epoch [2/10], Loss: 0.1238\n",
      "Epoch [3/10], Loss: 0.1190\n",
      "Epoch [4/10], Loss: 0.1172\n",
      "Epoch [5/10], Loss: 0.1148\n",
      "Epoch [6/10], Loss: 0.1131\n",
      "Epoch [7/10], Loss: 0.1109\n",
      "Epoch [8/10], Loss: 0.1087\n",
      "Epoch [9/10], Loss: 0.1059\n",
      "Epoch [10/10], Loss: 0.1033\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "num_epochs = 10\n",
    "num_classes = 19\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = 0.0\n",
    "\n",
    "    for images, masks in train_loader:\n",
    "\n",
    "        # Ensurs the pixel values in masks are in the range [0, num_classes - 1].\n",
    "        masks = torch.clamp(masks, 0, num_classes-1)\n",
    "        images, masks = images.float().to(device), masks.long().to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = loss_func(outputs, masks)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {train_loss/len(train_loader):.4f}\")\n",
    "\n",
    "print('Training complete!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GbNrNjzTod0v",
    "outputId": "5cfa5800-4770-4b78-ca4e-682168a242fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved!\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), 'semantic_segmentation_model.pt')\n",
    "print('Model saved!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QU_ZZoi1xV1Z",
    "outputId": "2cdf7a53-6fef-4f8f-c23f-be65d340941f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Accuracy: 97.84%\n"
     ]
    }
   ],
   "source": [
    "# Validation loop\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.eval()\n",
    "    correct_pixels = 0\n",
    "    total_pixels = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, masks in val_loader:\n",
    "            masks = torch.clamp(masks, 0, num_classes-1)\n",
    "            images, masks = images.float().to(device), masks.long().to(device)\n",
    "\n",
    "            outputs = model(images)  # Model prediction (logits)\n",
    "            predictions = torch.argmax(outputs, dim=1)  # Get class with highest probability\n",
    "\n",
    "            correct_pixels += (predictions == masks).sum().item()\n",
    "            total_pixels += masks.numel()  # Total number of pixels\n",
    "\n",
    "accuracy = correct_pixels / total_pixels * 100  # Convert to percentage\n",
    "print(f\"Final Accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
